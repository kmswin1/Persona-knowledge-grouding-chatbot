INFO:train_focus.py:Arguments: Namespace(bart_model_path='facebook/bart-base', dev_dataset_cache='data/focus_cache.tar.gz', dev_dataset_path='data/valid_focus.json', device='cuda', eval_before_start=False, flag='E2_L10_WO_PS', fp16='', gpt2_model_path='gpt2', gpu_start_num=1, gradient_accumulation_steps=16, incontext=True, inference=False, kn_coef=1.0, lm_coef=10.0, local_rank=-1, lr=6.25e-05, max_history=1, max_norm=1.0, model_name='GPT2', n_epochs=2, ps_coef=0.0, random_knowledge=False, seed=19950604, test_infer=False, train_batch_size=4, train_dataset_cache='data/focus_cache.tar.gz', train_dataset_path='data/train_focus.json', valid_batch_size=1)
INFO:train_focus.py:Prepare tokenizer, pretrained model and optimizer.
Some weights of GPT2PK_ctxt were not initialized from the model checkpoint at gpt2 and are newly initialized: ['summary.summary.weight', 'attn1.bias', 'attn2.bias', 'attn2.weight', 'concat_summary.summary.weight', 'concat_summary.summary.bias', 'summary.summary.bias', 'attn1.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
